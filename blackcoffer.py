# -*- coding: utf-8 -*-
"""BlackCoffer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10G0H6BEHIn-pFLPGM29UuchMnJ1s_k-q
"""

#importing dependencies

from bs4 import BeautifulSoup as bs
import requests

import urllib.request

import pandas as pd

import re

import nltk
 nltk.download('punkt')

import numpy as np

from nltk.tokenize import word_tokenize, sent_tokenize

file = pd.read_excel('Input.xlsx')

#extracting content and titles from urls

def extract_title_content(df):
    ''' Extracting title and content from urls'''
    from bs4 import BeautifulSoup
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',
      }

    URL = df.URL
    r = requests.get(URL, headers= headers)
    soup = BeautifulSoup(r.content, 'html.parser')

    text = soup.find_all(text=True)
    #set([t.parent.name for t in text])

    blacklist = [
        '[document]','noscript','header','html',
        'meta','head', 'input','script','a',
     'article','aside','body','button','div',
     'figcaption','footer','form', 'h1',
     'head', 'header', 'html', 'li',
     'link', 'meta',  'pre', 'script',
     'span', 'strong', 'style', 'time', 'ul']
    for t in text:
        if t.parent.name not in blacklist:
            if t.parent.name == 'title':
                title=t
            else:
                content = t

    df['Title'] = title
    df['content'] =content
    return df

file=file.apply(extract_title_content, axis=1)

# saving file with content and title
file.to_csv('file.csv', index=False)

# creating list of Stopwords
with open ("StopWords_Generic.txt", 'r') as f:
    stop = f.readlines()
    stopwords =[]
    for word in stop:
        stopwords.append(word)

# loading saved file in earlier steps
file=pd.read_csv('file.csv')

#creating Postive Negative words Dictionary

dic = pd.read_csv('MasterDictionary.csv')

positive= dic.Word[dic.Positive!=0].to_list()
negative = dic.Word[dic.Negative!=0].to_list()

new_dic=dict()
new_dic['Positive'] = [word for word in positive if word not in stopwords]
new_dic['Negative'] =[word for word in negative if word not in stopwords ]

def calculating_scores(row):
    '''Analysis of text'''
    tokens = word_tokenize(row.content)
    total_unclean_words = len(tokens)
    tokens= [x.upper() for x in tokens]
    #removing punctuatuion
    tokens = [re.sub(r'[^\w\s]', '', token) for token in tokens]
    pos=0; neg=0 ; ch =0
    for token in tokens:
        ch += len(token)
        if token in new_dic['Positive']:
            pos +=1
        elif  token in new_dic['Negative']:
            neg -=1  
    neg = neg * (-1)
    polarity = (pos-neg)/ ((pos + neg) + 0.000001) 
    cont = row.content
    word = cont.split()
    cont_clean=[token for token in tokens if token not in stopwords]
    tot_clean_words = len(cont_clean)
    
    sentence = sent_tokenize(row.content)
    tot_sent = len(sentence)
    av_sent_len = tot_clean_words/(tot_sent + 0.000001)
    
    count_complexWord, avg_syllablePerWord = count_complex(row.content)
    perc_complex = count_complexWord / (tot_clean_words+ 0.000001)
    fogIndex = 0.4 * (av_sent_len + perc_complex)
    
    subjectivity = (pos + neg)/ (tot_clean_words + 0.000001)
    
    avg_wordPerSent = total_unclean_words/(tot_sent + 0.000001)
    
    av_word_length = ch/(total_unclean_words+ 0.000001)
    count_perspronoun = count_pronoun(row.content)
    
    
    row['POSITIVE SCORE'] = pos
    row['NEGATIVE SCORE'] =  neg
    row['POLARITY SCORE'] =  polarity
    row['SUBJECTIVITY SCORE'] =  subjectivity
    row['AVG SENTENCE LENGTH'] =  av_sent_len
    row['PERCENTAGE OF COMPLEX WORDS'] =  perc_complex
    row['FOG INDEX'] =  fogIndex
    row['AVG NUMBER OF WORDS PER SENTENCE'] =  avg_wordPerSent
    row['COMPLEX WORD COUNT'] =  count_complexWord
    row['WORD COUNT'] =  tot_clean_words
    row['SYLLABLE PER WORD'] =  avg_syllablePerWord
    row['PERSONAL PRONOUNS'] =  count_perspronoun
    row['AVG WORD LENGTH'] =  av_word_length
    
    return row

def count_complex(text):
    '''counting syllables and complex words'''
    complx= []
    vowel_l = []
    tokens = word_tokenize(text)
    tokens= [x.lower() for x in tokens]
    for token in tokens:
        vowel = len(re.findall("e|a|i|o|u", token))-len(re.findall("\w+ed|s$", token))
        vowel_l.append(vowel)
        if vowel > 2 :
            complx.append(token)
#     print(tokens)
#     print(vowel,complx)        
    return len(complx) , np.mean(vowel_l)

def count_pronoun(text):
    '''counting personal pronoun (excluding country US)'''
    tot = 0
    pronounRegex = re.compile(r'\b(I|we|my|ours|(?-i:us))\b',re.I)
    pronoun = pronounRegex.findall(text)
    total = len(pronoun)
      
    return total

file.head()

new = file.apply(calculating_scores, axis=1)

new.head()

new.to_csv('output_Rashi.csv')

